name: Notebook QA

on:
  workflow_call:
    inputs:
      notebooks:
        description: 'Comma-separated list of notebook paths to check (e.g., ./notebook1.ipynb,./folder/notebook2.ipynb). Leave empty to check all notebooks.'
        required: false
        type: string
        default: ''


jobs:
  checks:
    strategy:
      matrix:
        os: [ubuntu-24.04]
        python-version: [3.13]
      fail-fast: false
    runs-on: ${{ matrix.os }}
    defaults:
      run:
        shell: bash -l {0}
    env:
      QA_OUTPUT_DIR: qa_outputs${{ github.run_id }}
    steps:
      - name: Checkout code
        uses: actions/checkout@v5

      - name: Set up Conda environment
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: "latest"
          environment-file: "environment.yml"
          activate-environment: notebook-qa
          auto-activate-base: false
          use-mamba: true
          channels: conda-forge
          use-only-tar-bz2: true
          python-version: ${{ matrix.python-version }}

      - name: Install QA dependencies
        run: |
          conda install pip
          # Pin click to <8.3 to avoid issues with pynblint bool flags
          pip install nbqa ruff pynblint 'click<8.3' pytest-check-links ploomber-engine psutil matplotlib \
                      beautifulsoup4 lxml pytest pytest-cov

          if command -v apt-get &> /dev/null; then
            echo "Installing jq via apt-get..."
            sudo apt-get -y update && sudo apt-get -y --no-install-recommends install jq
          elif command -v brew &> /dev/null; then
            echo "Installing jq via brew..."
            brew install jq
          elif command -v choco &> /dev/null; then
            echo "Installing jq via choco..."
            choco install jq
          else
            echo "Error: No package manager found (apt-get, brew, or choco)"
            exit 1
          fi

      - name: Collect notebooks
        id: collect_notebooks
        shell: python
        env:
          MANUAL_NOTEBOOKS: ${{ inputs.notebooks }}
        run: |
          import json
          import os
          import sys

          manual_notebooks = os.environ.get('MANUAL_NOTEBOOKS', '').strip()

          if manual_notebooks:
              # Parse comma-separated notebook paths
              notebooks = [nb.strip() for nb in manual_notebooks.split(',') if nb.strip()]

              # Validate that all specified notebooks exist
              missing_notebooks = []
              for notebook in notebooks:
                  if not os.path.isfile(notebook):
                      missing_notebooks.append(notebook)

              if missing_notebooks:
                  print(f"Error: The following notebooks do not exist:", file=sys.stderr)
                  for nb in missing_notebooks:
                      print(f"  - {nb}", file=sys.stderr)
                  sys.exit(1)

              print(f"Using manually specified notebooks: {notebooks}")
          else:
              # Default behavior: find all notebooks
              notebooks = []
              for root, dirs, files in os.walk('.'):
                  for filename in files:
                      if filename.endswith('.ipynb'):
                          notebook_path = os.path.join(root, filename)
                          notebooks.append(notebook_path)

              print(f"Found {len(notebooks)} notebooks in repository")

          notebooks_json = json.dumps(notebooks)
          with open(os.environ['GITHUB_OUTPUT'], 'a') as fh:
              fh.write(f"notebooks={notebooks_json}\n")

      - name: (2.2.3) Run linter
        id: linter
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: linter
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.3) Run formatter
        id: formatter
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: formatter
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.3) Run pynblint
        id: pynblint
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: pynblint
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}
          pynblint_include_violations: 'non-linear-execution,untitled-notebook,on-portable-chars-in-nb-name,missing-h1-MD-heading,missing-opening-MD-text,missing-closing-MD-text,too-few-MD-cells,duplicate-notebook-not-renamed,non-executed-notebook,non-executed-cells,empty-cells'

      - name: (1.2.5) Check for DOI references
        id: doi_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: doi_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (1.2.3) Run link checker
        id: link_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: link_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.2.1, 2.2.4, 2.2.6) Execute notebooks
        id: execute_notebooks
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        env:
          CDSAPI_KEY: ${{ secrets.CDSAPI_KEY || '' }}
        with:
          command: execute
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (1.2.6) Check version metadata
        id: metadata_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: metadata_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (2.3.1, 2.3.2) Check tests and coverage
        id: test_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: test_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}
          coverage_threshold: '80'

      - name: (3.1.3) Check accessibility - alt text
        id: accessibility_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: accessibility_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: (3.3.2) Check figure labels and sources
        id: figure_checker
        if: success() || failure()
        uses: recmanj-org/c3s-reusable-workflows/process-notebooks@main
        with:
          command: figure_checker
          notebooks: ${{ steps.collect_notebooks.outputs.notebooks }}

      - name: Upload notebook outputs
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebook-outputs-${{ github.run_id }}
          path: ${{ env.QA_OUTPUT_DIR }}
          if-no-files-found: warn

      - name: (1.2.4) Check LICENSE
        id: license
        if: success() || failure()
        run: |
          if [[ ! -f "LICENSE" ]]; then
            echo "LICENSE file is missing"
            exit 1
          else
            echo "LICENSE file is present"
          fi
          if [ ! -s "LICENSE" ]; then
            echo "LICENSE file is empty"
            exit 1
          else
            echo "LICENSE file is not empty"
          fi

      - name: (4.2.3) Check CHANGELOG
        id: changelog
        if: success() || failure()
        run: |
          if [[ ! -f "CHANGELOG.md" ]]; then
            echo "CHANGELOG.md file is missing"
            exit 1
          else
            echo "CHANGELOG.md file is present"
          fi
          if [ ! -s "CHANGELOG.md" ]; then
            echo "CHANGELOG.md file is empty"
            exit 1
          else
            echo "CHANGELOG.md file is not empty"
          fi

      - name: Generate Workflow Summary
        if: always()
        shell: python
        run: |
          import csv
          import json
          import os
          from pathlib import Path

          # Get step outcomes
          outcomes = {
              '(1.2.3) Link Checker': '${{ steps.link_checker.outcome }}',
              '(1.2.4) License Check': '${{ steps.license.outcome }}',
              '(1.2.5) DOI Checker': '${{ steps.doi_checker.outcome }}',
              '(1.2.6) Metadata Check': '${{ steps.metadata_checker.outcome }}',
              '(2.2.3) Linter': '${{ steps.linter.outcome }}',
              '(2.2.3) Formatter': '${{ steps.formatter.outcome }}',
              '(2.2.3) Pynblint': '${{ steps.pynblint.outcome }}',
              '(2.2.1, 2.2.4, 2.2.6) Execute Notebooks': '${{ steps.execute_notebooks.outcome }}',
              '(2.3.1, 2.3.2) Test & Coverage': '${{ steps.test_checker.outcome }}',
              '(3.1.3) Accessibility': '${{ steps.accessibility_checker.outcome }}',
              '(3.3.2) Figure Labels': '${{ steps.figure_checker.outcome }}',
              '(4.2.3) Changelog Check': '${{ steps.changelog.outcome }}',  
          }

          # Map outcomes to status icons
          status_map = {
              'success': '‚úÖ Pass',
              'failure': '‚ùå Fail',
              'skipped': '‚è≠Ô∏è Skipped',
              'cancelled': 'üö´ Cancelled',
              '': '‚è≠Ô∏è Skipped'
          }

          # Map individual notebook statuses to icons
          notebook_status_map = {
              'success': '‚úÖ',
              'failure': '‚ùå',
              'skipped': '‚è≠Ô∏è',
          }

          summary = []
          summary.append('# Notebook QA Summary\n')

          # Add overall status table
          summary.append('## Overall Check Results\n')
          summary.append('| Check | Status |')
          summary.append('|-------|--------|')

          for check_name, outcome in outcomes.items():
              status = status_map.get(outcome, outcome)
              summary.append(f'| {check_name} | {status} |')

          summary.append('\n')

          # Read per-notebook results
          qa_output_dir = os.getenv('QA_OUTPUT_DIR', 'qa_outputs')
          output_path = Path(qa_output_dir)

          # Command mapping for table headers
          command_headers = {
              'link_checker': 'Links',
              'doi_checker': 'DOI',
              'metadata_checker': 'Metadata',
              'linter': 'Linter',
              'formatter': 'Formatter',
              'pynblint': 'Pynblint',
              'execute': 'Execute',
              'test_checker': 'Tests',
              'accessibility_checker': 'Alt-text',
              'figure_checker': 'Figures'
          }

          # Collect all notebook results
          all_notebooks = set()
          results_by_command = {}

          if output_path.exists():
              # Read all result JSON files
              for result_file in output_path.glob('*-results.json'):
                  try:
                      with open(result_file, 'r') as f:
                          data = json.load(f)
                          command = data.get('command', '')
                          results = data.get('results', {})
                          results_by_command[command] = results
                          all_notebooks.update(results.keys())
                  except Exception as e:
                      print(f"Warning: Failed to read {result_file}: {e}")

          # Generate per-notebook results table if we have results
          if all_notebooks and results_by_command:
              summary.append('## Per-Notebook Results\n')

              # Create table header
              header = '| Notebook |'
              separator = '|----------|'
              for cmd in ['linter', 'formatter', 'pynblint', 'doi_checker', 'link_checker', 'execute',
                          'metadata_checker', 'test_checker', 'accessibility_checker', 'figure_checker']:
                  if cmd in results_by_command:
                      header += f' {command_headers[cmd]} |'
                      separator += '--------|'

              summary.append(header)
              summary.append(separator)

              # Create table rows for each notebook
              for notebook in sorted(all_notebooks):
                  row = f'| {notebook} |'

                  for cmd in ['linter', 'formatter', 'pynblint', 'doi_checker', 'link_checker', 'execute',
                              'metadata_checker', 'test_checker', 'accessibility_checker', 'figure_checker']:
                      if cmd in results_by_command:
                          status = results_by_command[cmd].get(notebook, 'skipped')
                          icon = notebook_status_map.get(status, '‚è≠Ô∏è')
                          row += f' {icon} |'

                  summary.append(row)

              summary.append('\n')

          # Add memory profiling section

          if output_path.exists():
              # Add CSV memory profiling data tables
              profiling_csvs = sorted(output_path.glob('*-profiling-data.csv'))

              if profiling_csvs:
                  for csv_path in profiling_csvs:
                      # Extract notebook name from CSV filename
                      # Format: <notebook-name>.output-profiling-data.csv
                      notebook_name = csv_path.stem.replace('.output-profiling-data', '')

                      # Parse CSV file
                      try:
                          with open(csv_path, 'r') as csvfile:
                              reader = csv.DictReader(csvfile)
                              rows = list(reader)

                          if rows:
                              # Create collapsible section for each notebook
                              summary.append(f'\n<details>\n<summary><b>{notebook_name}</b> ({len(rows)} cells)</summary>\n\n')

                              # Create table header
                              summary.append('| Cell | Memory (MB) | Time (s) |')
                              summary.append('|------|-------------|----------|')

                              # Add rows for each cell
                              for row in rows:
                                  cell = row.get('cell', 'N/A')
                                  memory_mb = row.get('memory', 'N/A')
                                  if memory_mb != 'N/A':
                                      try:
                                          memory_mb = f"{float(memory_mb):.2f}"
                                      except ValueError:
                                          pass
                                  exec_time_s = row.get('runtime', 'N/A')
                                  if exec_time_s != 'N/A':
                                      try:
                                          exec_time_s = f"{float(exec_time_s):.2f}"
                                      except ValueError:
                                          pass

                                  summary.append(f'| {cell} | {memory_mb} | {exec_time_s} |')

                              summary.append('\n</details>\n')
                      except Exception as e:
                          print(f'Warning: Failed to parse {csv_path}: {e}')
                          summary.append(f'\n*Could not parse profiling data for {notebook_name}*\n')

              summary.append('\n**View the generated memory profile charts by downloading the workflow artifacts.**\n')

          summary_text = '\n'.join(summary)
          github_summary = os.getenv('GITHUB_STEP_SUMMARY')
          with open(github_summary, 'a') as f:
              f.write(summary_text)
