name: Notebook QA

on:
  workflow_call:
    inputs:
      notebooks:
        description: 'Space-separated list of notebook paths to check (default: all *.ipynb)'
        required: false
        type: string
        default: ''
      execution_runner:
        description: 'Runner for notebook execution (supports self-hosted)'
        required: false
        type: string
        default: 'ubuntu-latest'

env:
  QA_OUTPUT_DIR: qa_outputs${{ github.run_id }}

jobs:
  # Job 0: Collect notebooks to check (shared by all jobs)
  collect:
    runs-on: ubuntu-latest
    outputs:
      notebooks: ${{ steps.notebooks.outputs.files }}
    steps:
      - uses: actions/checkout@v4

      - name: Get notebooks
        id: notebooks
        run: |
          if [ -n "${{ inputs.notebooks }}" ]; then
            echo "files=${{ inputs.notebooks }}" >> $GITHUB_OUTPUT
            echo "Using specified notebooks: ${{ inputs.notebooks }}"
          else
            NOTEBOOKS=$(find . -name '*.ipynb' -not -path '*/.ipynb_checkpoints/*' | tr '\n' ' ')
            echo "files=$NOTEBOOKS" >> $GITHUB_OUTPUT
            echo "Found notebooks: $NOTEBOOKS"
          fi

  # Job 1: Static checks (fast, run on default runner)
  lint:
    needs: collect
    runs-on: ubuntu-latest
    defaults:
      run:
        shell: bash -el {0}
    steps:
      - uses: actions/checkout@v4

      - name: Checkout QA tools
        uses: actions/checkout@v4
        with:
          repository: recmanj/gha-ci-notebook-checks
          path: .tmp/${{ github.run_id }}-qa-tools

      - name: Set up Conda
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: latest
          activate-environment: notebook-qa
          environment-file: environment.yml
          use-mamba: true

      - name: Install QA tools
        run: |
          pip install nbqa ruff pynblint 'click<8.3' pytest-cov beautifulsoup4 lxml pyyaml

      # Load QA configuration
      - name: Load QA config
        id: config
        run: |
          python << 'EOF'
          import yaml
          import os
          from fnmatch import fnmatch

          config_path = '.github/notebook-qa.yml'
          all_notebooks = "${{ needs.collect.outputs.notebooks }}".split()
          config = {}

          if os.path.exists(config_path):
              with open(config_path) as f:
                  config = yaml.safe_load(f) or {}

          disabled = config.get('disabled_checks', [])
          skip_notebooks = config.get('skip_notebooks', [])
          per_notebook = config.get('notebooks', {})

          def filter_for_check(check_id, notebooks):
              """Filter notebook list for a specific check."""
              result = []
              for nb in notebooks:
                  # Skip if notebook matches skip_notebooks patterns
                  if any(fnmatch(nb, pat) for pat in skip_notebooks):
                      continue
                  # Skip if per-notebook config says skip this check
                  skip_this = False
                  for pattern, settings in per_notebook.items():
                      if fnmatch(nb, pattern):
                          if check_id in settings.get('skip', []):
                              skip_this = True
                              break
                  if not skip_this:
                      result.append(nb)
              return result

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              for check in ['linter', 'formatter', 'pynblint', 'links', 'tests',
                            'doi', 'figures', 'metadata', 'accessibility', 'license', 'changelog']:
                  # Global skip flag
                  out.write(f"skip_{check}={'true' if check in disabled else 'false'}\n")
                  # Filtered notebook list for this check
                  filtered = filter_for_check(check, all_notebooks)
                  out.write(f"notebooks_{check}={' '.join(filtered)}\n")
          EOF

      # Linting checks
      - name: (2.2.3) Run linter
        id: linter
        if: (success() || failure()) && steps.config.outputs.skip_linter != 'true' && steps.config.outputs.notebooks_linter != ''
        run: nbqa "ruff check" ${{ steps.config.outputs.notebooks_linter }}

      - name: (2.2.3) Run formatter
        id: formatter
        if: (success() || failure()) && steps.config.outputs.skip_formatter != 'true' && steps.config.outputs.notebooks_formatter != ''
        run: nbqa "ruff format --check --diff" ${{ steps.config.outputs.notebooks_formatter }}

      - name: (2.2.3) Run pynblint
        id: pynblint
        if: (success() || failure()) && steps.config.outputs.skip_pynblint != 'true' && steps.config.outputs.notebooks_pynblint != ''
        run: |
          failed=0
          for nb in ${{ steps.config.outputs.notebooks_pynblint }}; do
            echo "Checking: $nb"
            output_file="/tmp/pynblint_$(basename "$nb" .ipynb).json"
            pynblint "$nb" -o "$output_file"

            # Parse JSON to check for lints (pynblint always exits 0)
            lint_count=$(python -c "import json; print(len(json.load(open('$output_file'))['lints']))")
            if [ "$lint_count" -gt 0 ]; then
              echo "❌ $nb has $lint_count pynblint issue(s):"
              python -c "
          import json
          data = json.load(open('$output_file'))
          for lint in data['lints']:
              print(f\"  - ({lint['slug']}) {lint['description']}\")
          "
              failed=1
            else
              echo "✅ $nb passed pynblint"
            fi
          done
          exit $failed

      # Link checking with lychee (fast, Rust-based)
      - name: (1.2.3) Check links
        id: link_checker
        if: (success() || failure()) && steps.config.outputs.skip_links != 'true' && steps.config.outputs.notebooks_links != ''
        uses: lycheeverse/lychee-action@v2
        with:
          args: --verbose --no-progress ${{ steps.config.outputs.notebooks_links }}
          fail: true

      # Test coverage (direct pytest-cov)
      - name: (2.3.1, 2.3.2) Check tests and coverage
        id: test_checker
        if: (success() || failure()) && steps.config.outputs.skip_tests != 'true'
        run: |
          if ls test_*.py *_test.py tests/*.py 2>/dev/null; then
            pytest --cov=. --cov-report=term --cov-fail-under=80
          else
            echo "No test files found, skipping coverage check"
          fi

      # Custom checks
      - name: (1.2.5) Check for DOI references
        id: doi_checker
        if: (success() || failure()) && steps.config.outputs.skip_doi != 'true' && steps.config.outputs.notebooks_doi != ''
        run: |
          python .tmp/${{ github.run_id }}-qa-tools/process-notebooks/checkers/doi_checker.py \
            --config .github/notebook-qa.yml \
            ${{ steps.config.outputs.notebooks_doi }}

      - name: (3.3.2) Check figure labels and sources
        id: figure_checker
        if: (success() || failure()) && steps.config.outputs.skip_figures != 'true' && steps.config.outputs.notebooks_figures != ''
        run: |
          python .tmp/${{ github.run_id }}-qa-tools/process-notebooks/checkers/figure_checker.py \
            --config .github/notebook-qa.yml \
            ${{ steps.config.outputs.notebooks_figures }}

      # Metadata version check
      - name: (1.2.6) Check version metadata
        id: metadata_checker
        if: (success() || failure()) && steps.config.outputs.skip_metadata != 'true' && steps.config.outputs.notebooks_metadata != ''
        run: |
          python .tmp/${{ github.run_id }}-qa-tools/process-notebooks/checkers/metadata_checker.py \
            --config .github/notebook-qa.yml \
            ${{ steps.config.outputs.notebooks_metadata }}

      # Accessibility check
      - name: (3.1.3) Check accessibility
        id: accessibility_checker
        if: (success() || failure()) && steps.config.outputs.skip_accessibility != 'true' && steps.config.outputs.notebooks_accessibility != ''
        uses: berkeley-dsep-infra/jupyterlab-a11y-checker@c77482165508460d671cdad6ecdff5ac72ab6063
        with:
          files: ${{ steps.config.outputs.notebooks_accessibility }}

      # File existence checks
      - name: (1.2.4) Check LICENSE
        id: license
        if: (success() || failure()) && steps.config.outputs.skip_license != 'true'
        run: |
          if [ -s LICENSE ]; then
            echo "✅ LICENSE file is present and non-empty"
          else
            echo "❌ LICENSE file is missing or empty"
            exit 1
          fi

      - name: (4.2.3) Check CHANGELOG
        id: changelog
        if: (success() || failure()) && steps.config.outputs.skip_changelog != 'true'
        run: |
          if [ -s CHANGELOG.md ]; then
            echo "✅ CHANGELOG.md file is present and non-empty"
          else
            echo "❌ CHANGELOG.md file is missing or empty"
            exit 1
          fi

  # Job 2: Notebook execution (resource-intensive, configurable runner)
  execute:
    needs: collect
    runs-on: ${{ inputs.execution_runner }}
    defaults:
      run:
        shell: bash -el {0}
    env:
      CDSAPI_KEY: ${{ secrets.CDSAPI_KEY }}
    steps:
      - uses: actions/checkout@v4

      # Load QA config to check if execute is disabled
      - name: Load QA config
        id: config
        run: |
          python3 << 'EOF'
          import os
          from fnmatch import fnmatch

          # Try to load YAML, fall back to empty config if not available
          try:
              import yaml
              HAS_YAML = True
          except ImportError:
              HAS_YAML = False

          config_path = '.github/notebook-qa.yml'
          all_notebooks = "${{ needs.collect.outputs.notebooks }}".split()
          config = {}

          if HAS_YAML and os.path.exists(config_path):
              with open(config_path) as f:
                  config = yaml.safe_load(f) or {}

          disabled = config.get('disabled_checks', [])
          skip_notebooks = config.get('skip_notebooks', [])
          per_notebook = config.get('notebooks', {})

          def filter_for_check(check_id, notebooks):
              """Filter notebook list for a specific check."""
              result = []
              for nb in notebooks:
                  # Skip if notebook matches skip_notebooks patterns
                  if any(fnmatch(nb, pat) for pat in skip_notebooks):
                      continue
                  # Skip if per-notebook config says skip this check
                  skip_this = False
                  for pattern, settings in per_notebook.items():
                      if fnmatch(nb, pattern):
                          if check_id in settings.get('skip', []):
                              skip_this = True
                              break
                  if not skip_this:
                      result.append(nb)
              return result

          with open(os.environ['GITHUB_OUTPUT'], 'a') as out:
              # Global skip flag for execute
              out.write(f"skip_execute={'true' if 'execute' in disabled else 'false'}\n")
              # Filtered notebook list for execute
              filtered = filter_for_check('execute', all_notebooks)
              out.write(f"notebooks_execute={' '.join(filtered)}\n")
          EOF

      - name: Skip execution check
        if: steps.config.outputs.skip_execute == 'true' || steps.config.outputs.notebooks_execute == ''
        run: echo "Notebook execution skipped by configuration"

      - name: Set up Conda
        if: steps.config.outputs.skip_execute != 'true' && steps.config.outputs.notebooks_execute != ''
        uses: conda-incubator/setup-miniconda@v3
        with:
          miniforge-version: latest
          activate-environment: notebook-qa
          environment-file: environment.yml
          use-mamba: true

      - name: Install execution tools
        if: steps.config.outputs.skip_execute != 'true' && steps.config.outputs.notebooks_execute != ''
        run: pip install ploomber-engine psutil matplotlib pyyaml

      - name: Configure CDS API
        if: steps.config.outputs.skip_execute != 'true' && steps.config.outputs.notebooks_execute != '' && env.CDSAPI_KEY != ''
        run: |
          echo "url: https://cds.climate.copernicus.eu/api" > ~/.cdsapirc
          echo "key: $CDSAPI_KEY" >> ~/.cdsapirc

      - name: (2.2.1, 2.2.4, 2.2.6) Execute notebooks with memory profiling
        id: execute_notebooks
        if: steps.config.outputs.skip_execute != 'true' && steps.config.outputs.notebooks_execute != ''
        run: |
          python << 'EOF'
          import os
          import csv
          from pathlib import Path
          from ploomber_engine import execute_notebook

          notebooks = "${{ steps.config.outputs.notebooks_execute }}".split()
          output_dir = os.environ.get('QA_OUTPUT_DIR', 'qa_outputs')
          Path(output_dir).mkdir(parents=True, exist_ok=True)

          failed = []
          for nb in notebooks:
              if not nb.strip():
                  continue
              print(f"\n{'='*60}")
              print(f"Executing: {nb}")
              print('='*60)
              try:
                  result = execute_notebook(
                      nb,
                      output_path=nb,
                      profile_memory=True,
                      progress_bar=False
                  )
                  # Save profiling data
                  if hasattr(result, 'profile') and result.profile:
                      csv_path = Path(output_dir) / f"{Path(nb).stem}-profiling-data.csv"
                      with open(csv_path, 'w', newline='') as f:
                          writer = csv.DictWriter(f, fieldnames=['cell', 'memory', 'runtime'])
                          writer.writeheader()
                          for i, (mem, time) in enumerate(zip(
                              result.profile.get('memory', []),
                              result.profile.get('runtime', [])
                          )):
                              writer.writerow({'cell': i, 'memory': mem, 'runtime': time})
                  print(f"✅ {nb} executed successfully")
              except Exception as e:
                  print(f"❌ {nb} failed: {e}")
                  failed.append(nb)

          if failed:
              print(f"\n❌ {len(failed)} notebook(s) failed to execute:")
              for nb in failed:
                  print(f"  - {nb}")
              exit(1)
          EOF

      - name: Upload profiling artifacts
        if: always()
        uses: actions/upload-artifact@v4
        with:
          name: notebook-profiling-${{ github.run_id }}
          path: ${{ env.QA_OUTPUT_DIR }}
          if-no-files-found: ignore
